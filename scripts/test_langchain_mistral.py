# ğŸ§  On importe le wrapper de LangChain pour appeler une API type OpenAI
from langchain.chat_models import ChatOpenAI

# ğŸ“© On importe le format du message "humain", comme dans ChatGPT (role=user)
from langchain.schema import HumanMessage

# ğŸ”Œ Connexion au serveur LocalAI qui tourne en local sur le port 8080
llm = ChatOpenAI(
    base_url="http://localhost:8080/v1",  # ton API locale (LocalAI)
    api_key="not-needed",                 # LocalAI ne vÃ©rifie pas la clÃ©
    model="mistral"                       # nom du modÃ¨le tel que dÃ©fini dans config.yaml
)

# ğŸ§¾ CrÃ©ation du message Ã  envoyer au modÃ¨le (comme un prompt dans ChatGPT)
messages = [
    HumanMessage(content="Je suis un nouveau papa. Comment puis-je aider ma femme qui vient dâ€™accoucher ?")
]

# ğŸ¤– Envoi du prompt au modÃ¨le Mistral via LangChain â†’ appel API en local
response = llm(messages)

# ğŸ–¨ï¸ Affichage de la rÃ©ponse du modÃ¨le
print("RÃ©ponse de DadAI (Mistral) :\n")
print(response.content)