{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“¦ Install missing libraries if needed\n",
    "!pip install transformers peft bitsandbytes --quiet\n",
    "\n",
    "# ========================\n",
    "# ðŸ”¥ Setup\n",
    "# ========================\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# ðŸ“‚ Paths\n",
    "base_model = \"mistralai/Mistral-7B-v0.1\"\n",
    "lora_weights_path = \"outputs/lora_weights\"\n",
    "\n",
    "# ðŸ§ª Test prompts\n",
    "test_prompts = [\n",
    "    \"My baby keeps crying when I put her down. Any advice?\",\n",
    "    \"I yelled at my kid today. Feeling horrible.\",\n",
    "    \"How do you keep your marriage strong after having kids?\",\n",
    "    \"My 3 y/o won't eat anything but nuggets. Help?\",\n",
    "    \"I just need a break. Is it bad to want one?\"\n",
    "]\n",
    "\n",
    "# ========================\n",
    "# ðŸš€ Load Model + LoRA\n",
    "# ========================\n",
    "print(\"ðŸ”„ Loading model...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model, quantization_config=bnb_config, device_map=\"auto\")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, lora_weights_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… Model ready!\")\n",
    "\n",
    "# ========================\n",
    "# ðŸ§ª Batch Inference\n",
    "# ========================\n",
    "def generate_reply(prompt_text):\n",
    "    full_prompt = f\"Post Reddit : {prompt_text}\\n\\nDad's reply:\"\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    reply = decoded.split(\"Dad's reply:\")[-1].strip()\n",
    "    return reply\n",
    "\n",
    "# ========================\n",
    "# ðŸŽ¯ Test prompts\n",
    "# ========================\n",
    "print(\"\\nðŸ“¨ Running tests...\\n\")\n",
    "\n",
    "for idx, prompt_text in enumerate(test_prompts, 1):\n",
    "    reply = generate_reply(prompt_text)\n",
    "    print(f\"ðŸŸ° Prompt {idx}: {prompt_text}\")\n",
    "    print(f\"ðŸ¤– DadAI: {reply}\")\n",
    "    print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dadai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
