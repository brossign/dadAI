import os
import time
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
from datasets import load_dataset
from datetime import datetime

# ğŸ•“ Mesure du temps total
start_time = time.time()

try:
    # ğŸ“ Chemin vers les donnÃ©es HF (crÃ©Ã©es par prepare_dataset.py)
    data_path = "../data/cleaned_dataset.jsonl"
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"âŒ Dataset introuvable Ã  : {data_path}")

    # ğŸ§  Chargement du tokenizer
    model_name = "mistralai/Mistral-7B-v0.1"
    print("ğŸ”¡ Chargement du tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token

    # ğŸ“¦ Chargement du dataset
    print("ğŸ“¦ Chargement du dataset...")
    dataset = load_dataset("json", data_files=data_path, split="train")

    # âœ‚ï¸ Tokenisation
    def tokenize(example):
        return tokenizer(
            example["prompt"],
            text_target=example["completion"],
            truncation=True,
            padding="max_length",
            max_length=512,
        )

    print("âœ‚ï¸ Tokenisation en cours...")
    dataset = dataset.map(tokenize, remove_columns=["prompt", "completion"])

    # âš™ï¸ Configuration LoRA
    print("âš™ï¸ Configuration de LoRA...")
    lora_config = LoraConfig(
        r=8,
        lora_alpha=16,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )

    # ğŸš€ Chargement du modÃ¨le en 4-bit
    print("ğŸš€ Chargement du modÃ¨le 4-bit...")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        load_in_4bit=True,
        device_map="auto"
    )

    model = prepare_model_for_kbit_training(model)
    model = get_peft_model(model, lora_config)

    # ğŸ“š Data collator
    collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )

    # ğŸ ParamÃ¨tres d'entraÃ®nement
    run_name = f"dadAI-lora-{datetime.now().strftime('%Y%m%d-%H%M')}"
    training_args = TrainingArguments(
        output_dir="outputs",
        per_device_train_batch_size=4,
        gradient_accumulation_steps=2,
        num_train_epochs=3,
        learning_rate=2e-4,
        logging_dir="logs",
        logging_steps=10,
        save_strategy="epoch",
        bf16=True if torch.cuda.is_bf16_supported() else False,
        report_to="none",
        run_name=run_name
    )

    # ğŸ“Š EntraÃ®neur HF
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        tokenizer=tokenizer,
        data_collator=collator
    )

    # âœ… Lancement entraÃ®nement
    print("ğŸš€ EntraÃ®nement en cours...")
    trainer.train()

    # ğŸ’¾ Sauvegarde des poids LoRA
    print("ğŸ’¾ Sauvegarde des poids LoRA...")
    model.save_pretrained("outputs/lora_weights")

    elapsed = time.time() - start_time
    print(f"âœ… EntraÃ®nement terminÃ© en {elapsed:.2f} secondes.")

except Exception as e:
    print(f"âŒ Erreur pendant le fine-tuning : {e}")